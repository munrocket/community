# Precise TFJS

| Status        | (Proposed)                                           |
:-------------- |:---------------------------------------------------- |
| **RFC #**     | [190](https://github.com/tensorflow/community/issues/190)|
| **Author(s)** | Munrocket (munrocket@pm.me)                          |
| **Sponsor**   | *become my sponsor*                                  |
| **Updated**   | 2019-12-18                                           |

## Objective

TFJS version with emulated double precision that use double-word technique on GPU

## Motivation

TFJS have issues with precision [1110](https://github.com/tensorflow/tfjs/issues/1110),
[1209](https://github.com/tensorflow/tfjs/issues/1209)
and [so on](https://github.com/tensorflow/tfjs/search?p=1&q=precision&type=Issues).
WASÐœ can be used as alternative for flaot64 calculations, but it not use GPU parallelism.

## User Benefit

Better precision for trained models in browsers on PCs/Mobiles. GPU implementation usually
much faster than WASM on PC and consume less power on mobile phones.

## Design Proposal

Making a floating point expansion with double-word technique on GPU where size of word is float32 [1].
Final mantissa size will be equal to 2 x 22 = 44 bits in emulated float64, while in float64 - 52 bits.
If some devices have troubles with float32 correctness we can shrink size of used mantissa and perform
renormalization procedure after each floating point operation. [2]

### Alternatives Considered
* WASM implementation but it not so fast
* FMA operation supported only in CUDA

### Performance Implications
* Slower that ordinary implementation in ~10 times [according](http://blog.hvidtfeldts.net/index.php/2012/07/double-precision-in-opengl-and-webgl/) to mandelbrot benchmark.
But still faster than WASM with same precision.
* GPU memory consumption will increase in x2. Because we need to pack 2 float32 for each emulated float64.
* ~~Microbenchmarks not provided~~

### Dependencies
* Dependencies: none
* Dependent projects: none

### Engineering Impact
* Binary size and startup time will be the same. Test time increased little bit. Build time can be increased a lot
if source code will be autogenerated.
* For better maintaining, this implementation can be autogenerated from core implementation in future.

### Platforms and Environments
* Full support expected

### Best Practices, Tutorials and Examples
* It will change best practices for those who want more precision in model.
In this case you will need add another dependency that differ from current implementation,
all another code will be the same.
* This case will be added in documentation and existed tutorials.

### Compatibility
* It will be backwards and forwards compatible, because we can load stored model with lower precision
but probably it will be waste of time on a client machines.
* We need to consume SavedModel with better precision, probably we can take serialization format from CUDA ver.
* It will be not supported by TFLite at least for now
~~How will it work with distribution strategies?~~
~~How will it interact with tf.function?~~

### User Impact
User need to train more precise models and use precise tfjs-core version

### Questions and Discussion Topics
We can emulate float128 with double-double in CUDA backend with the same tehnique. It can work much faster
becasue FMA operation can improve double-word multiplication performance in x8.5 times [1].

### References
1. Mioara Joldes, Jean-Michel Muller, Valentina Popescu
 *Tight and rigourous error bounds for basic building blocks of double-word arithmetic.*, 2017.
2. Mioara Joldes, Olivier Marty, Jean-Michel Muller and Valentina Popescu.
*Arithmetic Algorithms for Extended Precision Using Floating-Point Expansions*, 2016
